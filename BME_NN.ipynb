{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BME_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing & Installing Libraries"
      ],
      "metadata": {
        "id": "Oj7_kVKW29ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Project to Googloe Drive\n",
        "from google.colab import drive\n",
        "import os, sys\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "# Path to Code Folders\n",
        "data_dir_path = \"/content/gdrive/MyDrive/BME_Design/PEAC_Code/WESAD\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TLR31BUsGKog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dacd5125-9fce-4ff2-9b06-78bce60666e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Analysis Libraries\n",
        "!pip install pyhrv\n",
        "!pip install nolds\n",
        "!pip install spectrum\n",
        "!pip install neurokit2\n",
        "!pip install scipy\n",
        "!pip install statistics\n",
        "!pip install tensorflow\n",
        "!pip install biosppy"
      ],
      "metadata": {
        "id": "yV422d_DTuxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8c6b8a-276c-4594-f2b1-7f2a797a0d25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyhrv in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: biosppy in /usr/local/lib/python3.7/dist-packages (from pyhrv) (0.8.0)\n",
            "Requirement already satisfied: spectrum in /usr/local/lib/python3.7/dist-packages (from pyhrv) (0.8.1)\n",
            "Requirement already satisfied: nolds in /usr/local/lib/python3.7/dist-packages (from pyhrv) (0.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyhrv) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyhrv) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pyhrv) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from biosppy->pyhrv) (1.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from biosppy->pyhrv) (1.15.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from biosppy->pyhrv) (4.1.2.30)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.7/dist-packages (from biosppy->pyhrv) (1.0.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from biosppy->pyhrv) (1.1.0)\n",
            "Requirement already satisfied: bidict in /usr/local/lib/python3.7/dist-packages (from biosppy->pyhrv) (0.22.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from biosppy->pyhrv) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->biosppy->pyhrv) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyhrv) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyhrv) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyhrv) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyhrv) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->pyhrv) (4.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from nolds->pyhrv) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nolds->pyhrv) (57.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->biosppy->pyhrv) (3.1.0)\n",
            "Requirement already satisfied: easydev in /usr/local/lib/python3.7/dist-packages (from spectrum->pyhrv) (0.12.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from easydev->spectrum->pyhrv) (0.4.4)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from easydev->spectrum->pyhrv) (6.6.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from easydev->spectrum->pyhrv) (4.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->easydev->spectrum->pyhrv) (0.7.0)\n",
            "Requirement already satisfied: nolds in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nolds) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nolds) (57.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from nolds) (0.16.0)\n",
            "Requirement already satisfied: spectrum in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spectrum) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from spectrum) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spectrum) (1.21.6)\n",
            "Requirement already satisfied: easydev in /usr/local/lib/python3.7/dist-packages (from spectrum) (0.12.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from easydev->spectrum) (4.8.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from easydev->spectrum) (6.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from easydev->spectrum) (0.4.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->spectrum) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->spectrum) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->spectrum) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->spectrum) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->spectrum) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->spectrum) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->easydev->spectrum) (0.7.0)\n",
            "Requirement already satisfied: neurokit2 in /usr/local/lib/python3.7/dist-packages (0.1.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from neurokit2) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->neurokit2) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->neurokit2) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neurokit2) (2022.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->neurokit2) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->neurokit2) (3.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n",
            "Requirement already satisfied: statistics in /usr/local/lib/python3.7/dist-packages (1.0.3.5)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.7/dist-packages (from statistics) (0.17.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.25.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: biosppy in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from biosppy) (1.4.1)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.7/dist-packages (from biosppy) (1.0.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from biosppy) (1.1.0)\n",
            "Requirement already satisfied: bidict in /usr/local/lib/python3.7/dist-packages (from biosppy) (0.22.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from biosppy) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from biosppy) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biosppy) (1.21.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from biosppy) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from biosppy) (1.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from biosppy) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->biosppy) (1.5.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->biosppy) (4.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->biosppy) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NDboZoeGaS01"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd  # reading/managing dataset\n",
        "import matplotlib.pyplot as plt  # plotting\n",
        "from scipy import signal \n",
        "import statistics as stat\n",
        "import glob\n",
        "import biosppy\n",
        "import neurokit2 as nk\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.model_selection import train_test_split  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Extraction and Preparation"
      ],
      "metadata": {
        "id": "RqX-8152gIy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PATHS TO WESAD DATA FOLDERS --- # \n",
        "folder_paths = [\n",
        "                \"/S2/S2\",\n",
        "                \"/S3/S3\",\n",
        "                \"/S4/S4\",\n",
        "                \"/S5/S5\",\n",
        "                \"/S6/S6\",\n",
        "                \"/S7/S7\",\n",
        "                \"/S8/S8\",\n",
        "                \"/S9/S9\",\n",
        "                \"/S10/S10\",\n",
        "                \"/S11/S11\",\n",
        "                \"/S13/S13\",\n",
        "                \"/S14/S14\",\n",
        "                \"/S15/S15\",\n",
        "                \"/S16/S16\",\n",
        "                \"/S17/S17\",\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "FkC22VyvlaNQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VARIABLE DECLARATION FOR DATASET --- # \n",
        "sampling_rate = 700"
      ],
      "metadata": {
        "id": "HxjLppG7c-iZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNCTION TO EXTRACT EVENT TIMES FOR DATA LABELS --- # \n",
        "\"\"\"@brief       :   function to extract information from WESAD quest csv\n",
        "   @params[in]  :   datapath - string path to data folder\n",
        "   @params[out] :   event_times - tuple list of start and end times for each data label\n",
        "   @params[out] :   anxiety_times - start and end time tuple for recording of anxiety time\n",
        "\"\"\"\n",
        "def get_event_times(datapath):\n",
        "  event_times = []                #< array to hold start and end times of recording events \n",
        "  anxiety_times = []              #< array to hold start and end tme stamps fo anxiety events\n",
        "\n",
        "  # Convert Quest information cav to pandas dataframe\n",
        "  quest_df = pd.read_csv(datapath + '_quest.csv')\n",
        "\n",
        "  # Extract Start and end times as strings\n",
        "  start_times = (((quest_df.iloc[1]).to_numpy()[0]).split(';'))[1:]\n",
        "  end_times = (((quest_df.iloc[2]).to_numpy()[0]).split(';'))[1:]\n",
        "\n",
        "  # Extract labels\n",
        "  state_order = (((quest_df.iloc[0]).to_numpy()[0]).split(';'))[1:]\n",
        "  order_idx = None\n",
        "\n",
        "  # Loop through values and pair start & end times\n",
        "  for val in range(len(start_times)):\n",
        "    try:\n",
        "      # Convert Start and end times from string (minutes) to float (sec)\n",
        "      start = float(start_times[val])*60\n",
        "      end = float(end_times[val])*60\n",
        "      event_times.append([start, end])\n",
        "      # Determine state in which stress readings were recorded\n",
        "      if str(state_order[val])[0] == str(\"T\"):\n",
        "        order_idx = val\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "  # Determine start and end times for anxiety events\n",
        "  if not order_idx == None:\n",
        "    anxiety_times = event_times[order_idx]\n",
        "\n",
        "  return event_times, anxiety_times"
      ],
      "metadata": {
        "id": "tMzKq3hVZ0MR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNCTION TO EXTRACT BIOSIGNALS FROM DICTIONARY OF RESPIBAN DATA --- # \n",
        "\"\"\"@brief       :   function to extract ecg, eda and acc data from pickled respiban dictionary\n",
        "   @params[in]  :   datapath - string path to data folder\n",
        "   @params[out] :   ecg_rawdata - array of ecg recording\n",
        "   @params[out] :   eda_rawdata - array of eda recordings\n",
        "   @params[out] :   acc_rawdata - array of accelerometer recordings\n",
        "   @params[out] :   data_len - length of arrays /number of recordings\n",
        "\"\"\"\n",
        "def extract_respiban_data(datapath):\n",
        "  # Unpickle respiban data package and convert to dictionary\n",
        "  data_dic = pd.read_pickle(datapath + '.pkl')\n",
        "  respiban_dic = data_dic['signal']['chest']\n",
        "\n",
        "  ecg_rawdata = respiban_dic['ECG']\n",
        "  eda_rawdata = respiban_dic['EDA']\n",
        "  acc_rawdata = respiban_dic['ACC']\n",
        "\n",
        "  data_len = ecg_rawdata.shape[0]\n",
        "\n",
        "  return ecg_rawdata, eda_rawdata, acc_rawdata, data_len"
      ],
      "metadata": {
        "id": "PCDiQFOwbj9y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNCTION TO FILTER ECG & EDA DATA --- # \n",
        "\"\"\"@brief       :   function to filter ECG and EDA data\n",
        "   @params[in]  :   ecg - raw ecg data\n",
        "   @params[in]  :   eda - raw eda data \n",
        "   @params[in]  :   sampling_rate - sampling rate of the data \n",
        "   @params[out] :   ecg_filtered  - filtered ecg data\n",
        "   @params[out] :   eda_filtered  - filtered eda data\n",
        "\"\"\"\n",
        "def filter_data(ecg,eda,sampling_rate):\n",
        "  # ECG Filtering\n",
        "  ecg_rawdata_features = biosppy.signals.ecg.ecg(signal=ecg, sampling_rate=sampling_rate)\n",
        "  ecg_filtered = ecg_rawdata_features['filtered']\n",
        "\n",
        "  # EDA Filtering\n",
        "  signals ,_ = nk.eda_process(eda,sampling_rate = sampling_rate)\n",
        "  eda_filtered = signals['EDA_Clean']\n",
        "\n",
        "  return ecg_filtered, eda_filtered "
      ],
      "metadata": {
        "id": "1AH0tEshvCNe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNCTION TO CONSTRUCT ARRAY OF TIME SERIES DATA --- # \n",
        "\"\"\"@brief       :   function to create an array of time values for respiban recordings\n",
        "   @params[in]  :   data_len - number of datapoints in each recording array\n",
        "   @params[in]  :   sampling_rate\n",
        "   @params[out] :   end_time  - timestamp of last recording\n",
        "   @params[out] :   dt  - timestep between recordings\n",
        "   @params[out] :   t - array of time series data\n",
        "\"\"\"\n",
        "def get_time_data(data_len, sampling_rate):\n",
        "  end_time = data_len/sampling_rate\n",
        "\n",
        "  t = np.linspace(0, end_time, data_len)\n",
        "\n",
        "  dt = t[0] - t[1]\n",
        "\n",
        "  return t"
      ],
      "metadata": {
        "id": "lKjS3-fop0b3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNCTION TO ADD LABELS TO DATA --- # \n",
        "\"\"\"@brief       :   function to create an array of time values for respiban recordings\n",
        "   @params[in]  :   data_len - number of datapoints in each recording array\n",
        "   @params[in]  :   sampling rate  - data sampling rate\n",
        "   @params[in]  :   t - array of time series data\n",
        "   @params[in]  :   anxiety_time - start and end time of anxiety event\n",
        "   @params[out] :   labels - array of labeled data for specifi time points\n",
        "                      0 = no anxiety, 1 = anxiety\n",
        "\"\"\"\n",
        "def label_data(data_len, sampling_rate, t, anxiety_time):\n",
        "  # create array to store label data\n",
        "  labels = np.zeros(data_len)\n",
        "\n",
        "  # determine indexes for start and end of anxiety events\n",
        "  start = int(anxiety_time[0]*sampling_rate)\n",
        "  end = int(anxiety_time[1]*sampling_rate)\n",
        "\n",
        "  # add labels\n",
        "  labels[start:end] = 1\n",
        "\n",
        "  return labels\n",
        "\n"
      ],
      "metadata": {
        "id": "c6lUAZx-g3c4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNCTION TO CREATE DATA FRAME FOR EACH PARTICIPANT / TIME SERIES --- # \n",
        "\"\"\"@brief       :   function to create a dataframe to store labelled biosignal data for each participant\n",
        "   @params[in]  :   ecg_data \n",
        "   @params[in]  :   eda_data\n",
        "   @params[in]  :   acc_data\n",
        "   @params[in]  :   time_data\n",
        "   @params[in]  :   labels\n",
        "   @params[out] :   s_df - structured datatframe consisting of a particpant's recorded data \n",
        "\"\"\"\n",
        "def create_subject_df(ecg_data, eda_data, acc_data, time_data, labels):\n",
        "\n",
        "\n",
        "  s_dic = {\n",
        "    'TIME'          : time_data,\n",
        "    'EDA_RESPIBAN'  : eda_data,\n",
        "    'ECG_RESPIBAN'  : ecg_data,\n",
        "    'X_RESPIBAN'    : acc_data[0],\n",
        "    'Y_RESPIBAN'    : acc_data[1],\n",
        "    'Z_RESPIBAN'    : acc_data[2],\n",
        "    'LABEL'         : labels\n",
        "  }\n",
        "\n",
        "  # Convert dictionary into dataframe\n",
        "  s_df = pd.DataFrame.from_dict(s_dic)\n",
        "\n",
        "  return s_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S2b_1YZ4ebgv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MAIN  --- # \n",
        "\n",
        "\n",
        "# Loop through folders and extrapolate data\n",
        "for path in folder_paths:\n",
        "  #Define Datapath to acess subject recordings\n",
        "  datapath = data_dir_path + path\n",
        "  print(path)\n",
        "\n",
        "  # Extract event times from quest csv\n",
        "  event_times, anxiety_times = get_event_times(datapath)\n",
        "  print(anxiety_times)\n",
        "\n",
        "  # Unpack and filter respiban data\n",
        "  ecg_rawdata, eda_rawdata, acc_rawdata, data_len = extract_respiban_data(datapath)\n",
        "  #ecg_filtered, eda_filtered = filter_data(ecg_rawdata, eda_rawdata, sampling_rate)\n",
        "  # Adjust numpy dimensions\n",
        "  ecg_rawdata = ecg_rawdata.flatten()\n",
        "  eda_rawdata = eda_rawdata.flatten()\n",
        "  acc_rawdata = np.transpose(acc_rawdata)\n",
        "\n",
        "  # Create array of time data\n",
        "  t = get_time_data(data_len, sampling_rate)\n",
        "\n",
        "  # Create array to store data labels\n",
        "  labels = label_data(data_len, sampling_rate, t, anxiety_times)\n",
        "\n",
        "  # Store Data for each participant in dictionary\n",
        "  s_df = create_subject_df(ecg_rawdata, eda_rawdata, acc_rawdata, t, labels)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX0xdVxfgybx",
        "outputId": "22070538-5797-4636-b7db-9553438a4c88"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/S2/S2\n",
            "[2373.0, 3018.0]\n",
            "/S3/S3\n",
            "[2289.0, 2949.0]\n",
            "/S4/S4\n",
            "[3672.0, 4329.0]\n",
            "/S5/S5\n",
            "[3660.0, 4323.0]\n",
            "/S6/S6\n",
            "[2469.0, 3135.0]\n",
            "/S7/S7\n",
            "[3126.0, 3786.0]\n",
            "/S8/S8\n",
            "[3366.0, 4044.0000000000005]\n",
            "/S9/S9\n",
            "[1887.0, 2550.0]\n",
            "/S10/S10\n",
            "[3258.0, 3993.0]\n",
            "/S11/S11\n",
            "[1947.0000000000002, 2655.0]\n",
            "/S13/S13\n",
            "[3312.6, 3987.0]\n",
            "/S14/S14\n",
            "[1990.8, 2671.8]\n",
            "/S15/S15\n",
            "[3248.4, 3960.0]\n",
            "/S16/S16\n",
            "[2058.0, 2761.8]\n",
            "/S17/S17\n",
            "[3601.2000000000003, 4335.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = s_df.iloc[:,1:6].values\n",
        "y = s_df.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "Eps4mSc165IL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Signal Processing, Pre-processing and Feature Scaling"
      ],
      "metadata": {
        "id": "T1b2l7X4f-Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNCTION TO SEGMENT THE DATA INTO TIME STEPS --- # \n",
        "\"\"\"@brief       :   function to break the data into segments based on the timestep input\n",
        "   @params[in]  :   x_traning - filtered training data \n",
        "   @params[in]  :   y_traning - filtered training data \n",
        "   @params[in]  :   timestep - number of previous obesevations to be considered when the NN makes an observation \n",
        "   @params[out] :   x_train  - training data in a 2D numpy array\n",
        "   @params[out] :   y_train  - training data labels \n",
        "\"\"\"\n",
        "def segment_data(x_training,y_training,timestep):\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "  for i in range(timestep, len(x_training)):\n",
        "\n",
        "    x_train.append(x_training[i-timestep:i, :])\n",
        "\n",
        "    y_train.append(y_training[i])\n",
        "  \n",
        "  x_train = np.array(x_train)\n",
        "  y_train = np.array(y_train)\n",
        "\n",
        "  return x_train, y_train"
      ],
      "metadata": {
        "id": "Z_zD0_Arlz0B"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNCTION TO CONVERT ALL OF THE DATA --- # \n",
        "\"\"\"@brief       :   function to do all of the data conversions and filtering \n",
        "   @params[in]  :   ecg_signal - raw ecg signal \n",
        "   @params[in]  :   eda_signal - raw eda signal\n",
        "   @params[in]  :   acc_signal - raw accelertaion signal\n",
        "   @params[in]  :   timestep - number of previous obesevations to be considered when the NN makes an observation \n",
        "   @params[out] :   x_training  - training data in a 2D numpy array\n",
        "   @params[out] :   y_training  - training data labels \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "64LZlZWwu8oi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "70946f6b-9a60-4d74-b37c-0ec3e1b5de95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@brief       :   function to do all of the data conversions and filtering \\n   @params[in]  :   ecg_signal - raw ecg signal \\n   @params[in]  :   eda_signal - raw eda signal\\n   @params[in]  :   acc_signal - raw accelertaion signal\\n   @params[in]  :   timestep - number of previous obesevations to be considered when the NN makes an observation \\n   @params[out] :   x_training  - training data in a 2D numpy array\\n   @params[out] :   y_training  - training data labels \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNCTION TO SPLIT THE DATAFRAME INTO TRAINING, TEST, & VALIDATION DATA --- #\n",
        "\"\"\"@brief       :   function to split the data into \n",
        "   @params[in]  :   x - x section of the dataframe\n",
        "   @params[in]  :   y - y section of the dataframe\n",
        "   @params[in]  :   frac1 - fraction of the data to store as test/validation data\n",
        "   @params[in]  :   frac2 - fraction of the data to store from test/validation data\n",
        "   @params[out] :   x_train  - x training data \n",
        "   @params[out] :   y_train - y training data\n",
        "   @params[out] :   x_test  - x test data \n",
        "   @params[out] :   y_test - y test data \n",
        "   @params[out] :   x_val  - x validation data \n",
        "   @params[out] :   y_test - y validation data \n",
        "\"\"\"\n",
        "def split_data(x,y,frac1,frac2):\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=frac1)\n",
        "  x_test, x_val, y_test, y_val = train_test_split(x_test,y_test, test_size =frac2)\n",
        "\n",
        "  return x_train,y_train,x_test,y_test, x_val, y_val"
      ],
      "metadata": {
        "id": "IYbLcAhs2Pd7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Construction"
      ],
      "metadata": {
        "id": "N5UljcIalc-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n"
      ],
      "metadata": {
        "id": "F4_v_6Qp-7CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "uWRXU9NUljEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "1U9FHIMClp72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "WMh1upKZga3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate the data into training, test, and validation\n",
        "frac1 = .4 # 40% of data store for test/validation\n",
        "frac2 = .5 # 50% of that 40% (so 20% of origional dataset) stored as validation data \n",
        "[x_training,y_training,x_test,y_test,x_val,y_val] = split_data(x,y,frac1,frac2)\n",
        "\n",
        "timestep = 700\n",
        "# Segment the data into timestep\n",
        "[x_train,y_train] = segment_data(x_training,y_training,timestep)\n",
        "# may have to reshape the data "
      ],
      "metadata": {
        "id": "iUBps_qC99qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_train = np.reshape(x_train, (x_train.shape[0],  x_train.shape[1], 1))"
      ],
      "metadata": {
        "id": "WfL8OoT2AVIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN\n",
        "rnn = Sequential()\n",
        "# specify number of neurons desired in LSTM layer \n",
        "units = 45 # given from an example online (will likely change)\n",
        "return_sequence = True #unless it is the last LSTM Layer in the model then it is False\n",
        "# specify the shape of the training data \n",
        "input_shape = x_train.shape[1]\n",
        "\n",
        "for i in [True, True, False]:\n",
        "\n",
        "    rnn.add(LSTM(units, return_sequences = i))\n",
        "\n",
        "    rnn.add(Dropout(0.2))\n",
        "# Output layer    \n",
        "rnn.add(Dense(units = 1))\n",
        "\n",
        "# Compile the RNN\n",
        "# May want to change the optimizer or loss here \n",
        "rnn.compile(optimizer = 'adam', loss = 'mean_squared_error')\n"
      ],
      "metadata": {
        "id": "s8n4mJztgaI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the RNN to the training data \n",
        "# specify epochs \n",
        "epochs = 10 # will change \n",
        "# specify batch_size\n",
        "batch_size = 2 # will change\n",
        "\n",
        "rnn.fit(x_train,y_train,epochs,batch_size)"
      ],
      "metadata": {
        "id": "KSKWlXiF0zuW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "caba0d60-ddf6-485a-ac13-3708c712c7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "  1796/248620 [..............................] - ETA: 12:36:05 - loss: 0.1065"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6f16f34c1572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m# will change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape test data \n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], \n",
        "\n",
        "                              x_test.shape[1], \n",
        "\n",
        "                                          1))"
      ],
      "metadata": {
        "id": "_sU5dLJsnhaO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}